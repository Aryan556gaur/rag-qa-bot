{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqZeT4BWuuO1"
      },
      "outputs": [],
      "source": [
        "# from google.colab import userdata (only in colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Libraries Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install -q PyPDF2 pinecone-client cohere\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import cohere\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "API keys Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COHERE_API_KEY = os.getenv('COHERE_API_KEY')\n",
        "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cohere Setup and example Working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PXY4UUHXr4S6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
            "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\hp\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
          ]
        }
      ],
      "source": [
        "# Example with Cohere for embedding generation\n",
        "\n",
        "co = cohere.Client(COHERE_API_KEY)\n",
        "texts = [\"Rag is retrieval augmented generation where response i sgenerated referencing the provided document\", \"machine learnig is divided in supervised, semi-supervised, reinforcement and unsupervise\", \"Lstm is good while dealing with longer context data while GRU is better for dealing with text classification\"]  # your dataset\n",
        "response = co.embed(texts=texts, model=\"embed-english-v2.0\")\n",
        "embeddings = response.embeddings  # List of embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PineCone setup and index creation as a database for storing mbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SmbjxpYEu9Yw"
      },
      "outputs": [],
      "source": [
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "def create_index():\n",
        "  pc.create_index(\n",
        "      name=\"example-index\",\n",
        "      dimension=4096,\n",
        "      metric=\"cosine\",\n",
        "      spec=ServerlessSpec(\n",
        "          cloud='aws',\n",
        "          region='us-east-1'\n",
        "      )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fpLY-TkdI4I4"
      },
      "outputs": [],
      "source": [
        "create_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data ingestion in Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "X3rY_r1d8XHL"
      },
      "outputs": [],
      "source": [
        "index = pc.Index(\"example-index\")\n",
        "\n",
        "# Upload embeddings to Pinecone\n",
        "for i, embed in enumerate(embeddings):\n",
        "    index.upsert([(str(i), embed)])  # Document IDs are i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pipeline integration and working through example queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLttMON8AEbp",
        "outputId": "342bb6f0-0161-4017-a4f2-91a51efaaa80"
      },
      "outputs": [],
      "source": [
        "# Embed the query\n",
        "query = [\"How does Rag work?\", \"what are subsets of machine learning\", \"differenece in lstm and gru\"]\n",
        "query_embed = co.embed(texts=[query], model=\"embed-english-v2.0\").embeddings[0]\n",
        "\n",
        "# Retrieve the closest document embeddings\n",
        "for i in query_embed:\n",
        "    top_k = 5\n",
        "    result = index.query(vector=i, top_k=top_k)\n",
        "    retrieved_docs = [texts[int(match['id'])] for match in result['matches']]\n",
        "\n",
        "    # Generate answer using Cohere or any generative model\n",
        "    context = \" \".join(retrieved_docs)\n",
        "\n",
        "    for i in query:\n",
        "        response = co.generate(prompt=f\"{context}\\n\\nQuestion: {i}\\nAnswer:\", max_tokens=100)\n",
        "        print(f\"Response: {response.generations[0].text}\")\n",
        "        print(f\"Retrieved texts: {retrieved_docs}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
